{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価指標(分類)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "・混同行列（Confusion matrix）\n",
    "・正解率(Accuracy)\n",
    "・適合率（Precision）\n",
    "・再現率（Recall）\n",
    "・F値（F-measure）\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [0,0,0,0,0,1,1,1,1,1] # 0をNegative 1をPositiveとする\n",
    "y_pred = [0,1,0,0,0,0,0,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cmatrix = confusion_matrix(y_test, y_pred)\n",
    "print(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN 4\n",
      "FP 1\n",
      "FN 2\n",
      "TP 3\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel() #　混同行列のそれぞれの結果を取得\n",
    "print(\"TN\", tn)\n",
    "print(\"FP\", fp)\n",
    "print(\"FN\", fn)\n",
    "print(\"TP\", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正解率(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy:',accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "tp, fn, fp, tn = confusion_matrix(y_test, y_pred).ravel() \n",
    "print((tp + tn) / (tp + fp + fn + tn)) # 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 適合率（Precision）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適合率とは、正事例と予測したもののなかで真の値が正事例の割合を表す指標です。\n",
    "（出力した結果がどの程度正解していたのかを表す指標）\n",
    "適合率を重視するときはFN(False Negative)が発生することが許容できるケースです。\n",
    "例えば、利用シーンは、レコメンドや検索結果などです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print('Precision:', precision_score(y_test,y_pred)) # 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再現率（Recall）\n",
    "再現率とは、真の値が正事例のもののなかで正事例と予測した割合を表す指標です。\n",
    "再現率を重視するときはFP(False Positive)が発生することが許容できるケースです。\n",
    "利用シーンは、がん診断などです。\n",
    "再現率は別名、感度やヒット率、真陽性率とも呼びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print('Recall:', recall_score(y_test,y_pred)) # 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F値（F-measure）\n",
    "適合率と再現率はトレードオフの関係にあるので、2つの指標をまとめた指標としてF値があります。\n",
    "F値は、適合率と再現率の調和平均によって計算されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('F1 score:', f1_score(y_test,y_pred)) # 0.67 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.70      0.70        10\n",
      "weighted avg       0.71      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多クラス分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "0.9791666666666666\n",
      "0.9717034521788342\n"
     ]
    }
   ],
   "source": [
    "# 警告文を非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1)\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(precision_score(y_test, pred,average='macro'))\n",
    "print(recall_score(y_test, pred,average='macro'))\n",
    "print(f1_score(y_test, pred,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価方法(交差検証)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・交差検証（Cross-validation）：個々のモデルの汎化性能を評価する手法  \n",
    "・グリッドサーチ（grid search）：機械学習のハイパーパラメータ探索の方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# scikit-learnのtrain_test_split()を使うことでデータを分けることが可能です。\n",
    "# 何も設定しない場合、トレーニングデータには75%、テストデータは25%で分割されます。(test_sizeが25%にデフォルトで設定されています。)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差検証(クロスバリデーション)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "\n",
    "def evaluate_cross_validation(clf, x, y, K):\n",
    "    cv = KFold(len(y), K,shuffle=True,random_state=0)\n",
    "    scores = cross_val_score(clf,x,y,cv=cv)\n",
    "    print(scores)\n",
    "    print (\"Mean score: {} (+/-{})\".format( np.mean (scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グリッドサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {'C': [1, 5, 10, 50],\n",
    "     'gamma': [0.001, 0.0001]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'C': [1, 5, 10, 50], 'gamma': [0.001, 0.0001]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(x,y,clf, show_accuracy=True,show_classification_report=True, show_confussion_matrix=True):\n",
    "    y_pred=clf.predict(x)\n",
    "    if show_accuracy:\n",
    "        print(\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y, y_pred)), \"\\n\")\n",
    "\n",
    "    if show_classification_report:\n",
    "        print(\"Classification report\")\n",
    "        print(metrics.classification_report(y, y_pred), \"\\n\")\n",
    "\n",
    "    if show_confussion_matrix:\n",
    "        print(\"Confussion matrix\")\n",
    "        print(metrics.confusion_matrix(y, y_pred),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
